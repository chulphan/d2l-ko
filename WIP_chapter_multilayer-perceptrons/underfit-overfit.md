# 모델 선택, 언터피팅(underfitting), 오버피팅(overfitting)

머신러닝 과학자로서 우리의 목표는 일반적인 패턴을 발견하는 것입니다. 예들 들어 우리는 유전적 표식자를 성인기의 치매 발병과 연관시키는 패턴을 배우고자 한다고 해봅니다. 각 사람의 유전자는 데이터셋의 사람들 사이에서 뿐만 아니라 지구상의 모든 사람들 사이에서 고유하게 식별됩니다.

어떤 사람을 대표하는 유전 식별자가 주어졌을 때, 우리의 모델이 "아, 이것은 밥이야"라고 간단하게 의식하고, {치매, 경도인지 장애, 건강함} 중에 하나 중의 밥에 속하는 분류를 결과로 출력하는 것을 원하지 않습니다. 오히려 우리의 목표는 우리의 학습 셋을 추출한 기본 인구의 규칙성들을 포착하는 패턴을 발견하는 것입니다. 우리가 이 노력에 성공한다면 이전에 한번도 만난적이 없는 개인들에 대해서도 성공적으로 위험을 평가할 수 있을 것입니다. *일반화를 하는* 패턴을 어떻게 발견하냐의 문제는 머신러닝의 근본적인 문제입니다.

모델을 학습실 때, 우리는 데이터의 적은 샘플들만을 사용한다는 것이 위험입니다. 가장 큰 공공 이미지 데이터셋은 대략 백만개 이미지들을 가지고 있습니다. 그리고 우리는 자주 수 천 또는 수 만개를 사용해서 학습을 해야합니다. 큰 병원 시스템에서 우리는 아마 수십만 병원 기록에 접근할 수 있을 것입니다. 유한한 샘플들을 사용할 때, 우리는 더 많은 데이터를 수집했을 때 더 이상 유효하지 않는 *명백한* 연관성을 발견할 위험을 감수합니다.

극단적인 병리학적 사례를 생각해 보겠습니다. 어떤 사람이 대출금을 상환할 것인지를 예측하는 것을 학습하기를 원하는 경우를 상상해 봅니다. 대출 기관이 당신을 데이터 과학자로 고용해서 100명의 신청자의 완전한 파일들을 전달합니다. 이 중 5명은 3년 안에 파산을 했었습니다. 현실적으로는 파일에는 소득, 직업, 신용 점수, 고용 기간 등 수백 가지의 잠재적인 특성들을 가지고 있을 수 있습니다. 더군다나 대출 기관이 당신에게 각 대출 신청자가 대출 대리인과 한 인터뷰의 비디오 자료도 전달한다고 하겠습니다.

데이터를 큰 디자인 행렬로 특성을 구성한 후에, 여러분은 일반 인구의 40%만이 파란 셔츠를 입는 반면에 파산한 5명의 지원자 모두 인터뷰 동안 파란색 셔츠를 입고 있었고는 사실을 발견했다고 가정합니다. 파산을 예측하는 모델을 학습시키면, 그 모델이 파란 셔츠를 입고 있는 것을 중요한 특성으로 활용하게될 가능성이 있습니다. 

사실 채무 불이행자들이 일반 사람들보다 파란색 처츠를 입을 가능성이 더 적을지라도 우리가 5명의 모든 채무 불이행자들이 파란 셔츠를 입고 있는 것을 관찰 할  $.4^5 = .01$ 의 가능성이 있습니다. 단지 5개의 채무 불이행자의 예와 수백 또는 수천 개의 특성을을 가지고, 여러분은 단지 임의의 기회 때문에 우리의 일을 완벽하게 예측할 수 있는 많은 특성들을 아마도 발견할 것입니다. 데이터가 무한히 주어진다면, 우리는 이런 *거짓* 연관들이 결국에는 사라질 것을 기대할 수 있습니다. 하지만, 우리는 이런 사치를 기대하기 어렵습니다.

잠재적인 분포에 학습되는 것보다 학습 데이터에 더 잘 학습되는 현상을 오버핏이라고 하고, 오버핏을 해결하는데 사용되는 기술들을 정규화(regularization)이라고 합니다. 앞 절들에서 우리는 Fashion-MNIST 데이터셋으로 실험을 하는 동안 이 현상을 목격했을지도 모릅니다. 만약 여러분이 실험을 하는 동안 모델 구조 또는 하이퍼파라미터들을 변경을 했다면, 충분한 노드들, 층들과 학습 epoch을 사용했을 때, 그 모델은 테스트 데이터에 대해서는 더 상황이 악화되는 반면에 학습셋에 대해서 완벽한 정확도에 결국은 도달할 수 있습니다.

## 학습 오류와 일반화 오류

이 현상을 보다 공식적으로 논의하기 위해서, 우리는 *학습 오류* 와 *일반화 오류* 를 구분하는 것이 필요합니다. 학습 오류는 학습 데이터셋에 대해서 계산되는 모델의 오류인 반면에 일반화 오류는 오리지날 샘플과 같은 데이터 분포에서 추가 데이터 포인트들의 무한한 스트림에 적용했을 때의 모델 오류의 기대값입니다. 

확률적으로는 *우리는 일반화 오류를 정확하게 계산하는 것을 절대로 할 수 없습니다*. 무한한 데이터의 가상의 스트림은 가상의 객체이기 때문입니다. 실제 상황에서는 우리의 모델을 학습 셋으로 사용되지 않은 것들로 구성된 임의로 선택된 데이터 포인들로 구성된 독립적인 테스트 셋에 적용해서 일반화 오류를 *추정* 해야합니다.

다음 세 개의 사고 실험들은 이 상황을 잘 설명하는데 도움을 줄 것입니다. 기말 고사 준비을 하는 어느 한 대학생을 생각해 봅시다. 부지런한 학생은 준비를 잘 할려고 애쓸 것이고, 이전 년도의 시험 문제를 풀면서 공부한 능력을 테스트할 것입니다. 그럼에도 불구하고 과거의 시험을 잘 푼다는 것은 중요한 순간에도 뛰어날 것을 보장하지 않습니다. 예를 들면, 그 학생은 시험 문제들의 답을 기계적으로 학습하면서 시험을 준비를 하고 있을지도 모릅니다. 이를 위해서 학생은 많은 것들을 암기해야 합니다. 그 학생은 과거 시험들의 답을 완벽하게 암기할 수 있습니다. 반면 다른 학생은 주어진 답들에 대한 이유를 이해하면서 시험 준비를 하고 있습니다. 대부분의 경우에 후자의 학생이 시험을 더 잘 치룹니다.

마찬가지로 질문에 답을 하기 위해서 단순히 참조 테이블을 사용하는 모델을 생각해 보겠습니다. 만약 허용된 입력들의 집합이 이산적이고 합리적으로 작다면, 이 접근법은 아마 *많은* 학습 예제를 본 후에 잘 작동할 것입니다. 하지만 이 모델은 전혀 본적이 없는 예제들이 주어졌을 때 임의로 추측하는 것보다 더 잘하는 능력은 없습니다. 현실에서는 모든 가능한 입력에 대한 답을 암기할 수 없을 만큼 너무 큽니다. 예를 들면, $28\times28$ 흑백 이미지를 생각해보세요. 각 픽셀이 $256$ 회색 스케일 값 중에 하나를 갖을 수 있다면, $256^784$ 개의 가능한 이미지가 존재합니다. 이 것이 의미하는 바는 우주의 원자의 개수보다 더 많은 저해상도의 회색 스케일의 썸네일 그기의 이미지가 있다는 것입니다. 만약 이 데이터를 갖게될지라도, 참조 테이블 저장을 절대로 감당할 수 없습니다.

마지막으로, 있을지도 모르는 어떤 개념적인 특성에 근거해서 동전 던지기(클래스 0: 앞 면, 클래스 1: 뒷 면)의 결과를 구분하려는 문제를 생각해보겠습니다. 우리가 어떤 알고리즘에 도달하던지 상관 없이 일반화 오류는 항상  $\frac{1}{2}$ 입니다. 하지만, 대부분 알고리즘에 대해서 우리가 어떤 특성도 갖고 있지 않을 지라도 뽑기의 운에 따라서 학습 오류가 상당히 작게되기를 기대할 것입니다. 데이터셋 {0, 1, 1, 1, 0, 1}을 고려해보겠습니다. 우리의 특성이 없는 (feature-less) 알고리즘은 항상 *주요한*  클래스 (이 샘플들의 경우에는 1)로 예측하게 될 것입니다. 이 경우, 항상 클래스 1을 예측하는 모델의 오류는 $\frac{1}{3}$ 이 될 것이고, 이는 일반화 오류보다 상당히 좋은 값입니다. 우리가 데이터의 양을 증가시키면, 앞 면의 비율이  $\frac{1}{2}$ 로 부터 크게 멀어질 확률이 줄어들 것이고, 우리의 학습 오류는 일반화 오류와 동일하게 될 것입니다.

### 통계적 학습 이론

일반화는 머신러닝에서 근본적인 문제이기 때문에, 많은 수학자와 이론가들이 이 현상을 설명하는 형식적인 이론들을 개발하는데 삶을 바쳤다는 것은 놀라운 일이 아닙니다. [에포니머스 이론(eponymous theorem)](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem) 에서, 클리벤코(Glivenko)와 칸델리(Cantelli)는 학습 오류가 일반화 오류에 수렴하는 속도를 도출했습니다. 연속된 창의적인 논문들에서 [뱁닉(Vapnik)와 체르보넨키스(Chervonenkis)](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory) 는 이 이론을 더 함수들의 더 일반적인 클래스들로 확장했습니다. 이 것은 [통계적 학습 이론(Statistical Learning Theory)](https://en.wikipedia.org/wiki/Statistical_learning_theory) 의 근본이 되었습니다.

우리가 지금까지 살펴 본 그리고 이 책의 대부분을 차지할 **표준 지도 학습 설정** 에서, 우리는 학습 데이터와 테스트 데이터가 *동일한* 분포로 부터 *독립적* 으로 추출된다고 가정합니다. 이를 우리는 흔히 i.i.d. (independently from identical distribution) 가정이라고 부릅니다. 이는 샘플을 추출하는 어떤 프로세스도 *메모리*가 없다는 것을 의미합니다. 즉, 두 번째로 추출된 예제와 세 번째로 추출된 예제가 두 번째와 2백만 번째로 추출된 샘플보다 더 연관성이 있지 않다는 것입니다.

좋은 머신러닝 과학자가 되기 위해서는 비판적으로 생각해야 하며, 이미 이 가정에 구멍을 뚤고 있어야하고, 이 가정이 실패하는 일반적인 사례를 만들어야 합니다. 여러분이 만약 UCSF (University of California, San Francisco)의 환자들의 자료를 수집해서 사망 위험 예측 모델을 학습시키고, 이 모델을 메사추세스 종합 병원의 환자에 적용한다면 어떨까요? 두 병원의 환자들의 분포는 단순히 동일하지 않습니다. 더군다나 추출은 시간에 관련되어 있을 수 있습니다. 우리가 트윗의 주제들을 분류한다면 어떨까요? 뉴스 주기는 독립에 대한 가정을 위반하는 주제에 대해서 시간적인 의존성을 야기할 수도 있습니다.

때로 우리는 i.i.d. 추정을 위반한 사소한 것에서 벗어날 수 있고, 우리 모델들은 계속 아주 잘 동작할 것입니다. 결국 거의 모든 실 세계의 어플리케이션은 i.i.d. 가정에 대한 사소한 위반을 포함하고 있고, 여전히 우리는 얼굴 인식, 음성 인식, 언어 번역 등을 하는 유용한 도구를 가지고 있습니다.

다른 위반들은 확실히 문제를 야기합니다. 예를 들어 얼굴 인식 시스템을 학습시킬려고 한다고 생각해보겠습니다. 우리는 이 모델을 주로 대학생들의 자료를 사용해서 학습을 시키고, 요양원에서 노인들을 모니터링하는 도구로 사용하려고 합니다. 대학생들은 노인들과 상당히 다르게 보이기 때문에 이것은 잘 동작하지 않을 가능성이 높습니다.

이 후 장들과 볼륨들에서 우리는 i.i.d. 가정을 위반할 때 일어나는 문제들에 대해서 논의하겠습니다. 지금은 i.i.d. 가정을 당연하게 여겨도 일반화를 이해하는 것은 엄청난 문제입니다. 게다가 딥 뉴럴 네트워크가 일반화를 잘하는지를 설명할 수 있는 정확한 이론적인 근거를 해명하는 것은 학습 이론에서 가장 위대한 마음을 계속해서 괴롭힙니다.

모델을 학습시킬 때, 우리는 가능한한 학습 데이터에 잘 맞는 함수를 찾으려고 합니다. 만약 그 함수가 진짜 연관에 대한 패턴을 찾는 것처럼 가짜 패턴을 찾을 만큼 유연하다면, 보지 못한 데이터에 대해서 잘 일반화하는 모델을 만들지 못하면서 *너무 잘* 동작할 것입니다. 이것이 바로 우리가 피하기를 원하거나 최소한 제어하려는 것입니다. 딥러닝에서 많은 기법들은 오버피팅을 방어라려는 발견적 방법이고 요령들입니다.

### 모델 복잡도

우리는 간단한 모델들과 많은 데이터가 있을 경우, 일반화 오류가 학습 오류와 비슷해 지기를 예상합니다. 반면에 모델이 복잡하고 데이터가 적을 때는, 학습 오류는 작아지지만, 일반화 오류는 커질 것을 예상합니다. 무엇이 정확하게 모델의 복잡성을 구성하는지는 복잡한 문제입니다. 모델이 일반화를 잘 할 수 있을지는 많은 것들에 의해서 영향을 받습니다. 예를 들면, 더 많은 파라미터를 갖는 모델이 더 복잡하다고 여기질 수 있고, 값의 범위가 더 넓은 파라미터를 갖는 모델이 더 복잡하다고 여겨질 수도 있습니다. 뉴럴 네트워크의 경우에는 학습을 더 오래한 모델이 더 복잡한 것이라고 생각될 수도 있고, 일찍 학습을 종료한 모델은 덜 복잡하다고 생각될 수도 있습니다.

상당히 다양한 모델 종류들 간의 복잡성을 비교하는 것은 어려운 일 수 있습니다. 예를 들면 결정 트리(decision tree)와 뉴럴 네트워크의 복잡성을 비교하는 것은 어렵습니다. 이런 경우, 간단한 경험의 법칙을 적용하는 것이 유용합니다. 통계학자들은 임의의 사실을 잘 설명하는 모델을 복잡하다고 하고, 제한적인 설명을 하는 능력을 갖으나 데이터를 여전히 잘 설명하는 모델은 진실에 좀 더 가깝다고 합니다.  철학에서 이것은 포퍼의 과학 이론의 허위 진술성([falsifiability](https://en.wikipedia.org/wiki/Falsifiability))과 밀접한 관련이 있습니다. 어떤 이론이 데이터에 적합하고, 오류를 입증할 수 있는 특정 테스트가 있다면, 그 이론을 좋다고 합니다. 모든 통계적 추정이 [post-hoc](https://en.wikipedia.org/wiki/Post_hoc)이기에 이는 매우 중요합니다. 즉, 우리는 어떤 사실을 관찰한 후에 추정을 합니다. 따라서, 관련 오류에 취약하게 됩니다. 자, 철학에 대해서는 충분히 이야기했으니, 더 구체적인 이슈를 살펴보겠습니다.

여러분이 이 장에 대한 직관을 갖을 수 있도록, 

이 장에서 여러분에게 직관을 주기 위해서, 우리는 모델 클래스의 일반화에 영향을 줄 수 있는 몇가지 요소들에 집중하겠습니다.

1. 튜닝이 가능한 파라미터의 개수. *자유도(degree of freedom)*라고 불리기도 하는 튜닝 가능한 파라미터의 수가 많을 경우, 모델이 오버피팅(overfitting) 에 더 취약한 경향이 있습니다.
1. 파라미터에 할당된 값. 가중치들이 넒은 범위의 값을 갖을 경우, 모델은 오버피팅(overfitting)에 더 취약할 수 있습니다.
1. 학습 예제의 개수. 모델이 간단할 지라도 학습 데이터가 한 개 또는 두 개인 경우에는 오버피팅(overfit) 되기가 아주 쉽습니다. 하지만, 수백만개의 학습 데이터를 이용해서 모델을 오버피팅(overfitting) 시키기 위해서는 모델이 아주 복잡해야 합니다.

## 모델 선택

머신 러닝에서 우리는 보통 몇 개의 모델 후보들을 평가한 후 최종 모델을 선택합니다. 이 프로세스를 우리는 모델 선택이라고 합니다. 때로는 비교 할 모델들은 근본적으로 다른 종류의 것일 수 있습니다. (예를 들면, 결정 트리와 선형 모델) 어떤 경우에는 다른 하이퍼파라미터 세팅으로 학습된 동일한 클래스의 모델들을 비교하기도 합니다.

다층 퍼셉트론을 예로 들면, 우리는 다른 개수의 은닉층을 갖는 모델, 다른 개수의 은닉 유닛을 갖는 모델, 각 은닉층에 적용되는 다른 종류의 활성화 함수들을 갖는 모델을 비교할 수 있습니다. 우리의 후보 모델들 중에 가장 좋은 모델을 결정하기 위해서 우리는 일반적으로 검증 데이터 셋을 사용합니다.

### 검증 데이터셋 

원칙적으로 모든 하이퍼파라미터들을 선택을 완료하기 전에는 데스트 셋을 사용해서는 안됩니다. 모델 선택 프로세스에서 테스트 데이터를 사용한다면, 테스트 데이터에 오버핏할 위험이 있습니다. 이 경우에 우리는 심각한 문제에 빠질 것입니다. 학습 데이터에 오버핏 될 경우, 우리를 정직하게 만들 수 있는 테스트 데이터를 이용한 평가가 항상 있습니다. 하지만, 테스트 데이터에 오버핏될 경우, 우리는 어떻게 이 사실을 알아낼 수 있을까요? 

따라서, 우리는 모델 선택 과정에서 테스트 데이터의 절대로 의존해서는 안됩니다. 그리고 여전히 우리는 학습 데이터에만 전적으로 의존해서 모델 선택을 해서는 안됩니다. 그 이유는 모델 학습에 사용한 데이터를 가지고 일반화 오류를 평가할 수 없기 때문입니다.

이 문제를 해결하는 일반적인 방법은 데이터를 학습, 테스트 셋과 더불어서 *검증* 셋을 포함한 3가지로 나누는 것입니다.

실제 응용에서는 이 그림은 모호해집니다. 이상적으로는 데스트 데이터는 가장 최고의 모델을 평가하거나 작은 개수의 모델을 서로 비교하기 위해서 단 한번 사용되야 하지만, 실제 테스트 데이터는 한번 사용한 다음에 거의 패기되지 않습니다. 매 실험 마다 새로운 테스트 셋을 만드는 것이 거의 어렵습니다.

그 결과는 검증 데이터와 테스트 데이터의 경계가 우려할 만큼 모호해지게 되는 어설픈 관습입니다. 이 책의 실험에서 별도로 말하지 않는 한, 우리는 진짜 테스트 데이터는 없이 학습 데이터와 검증 데이터를 사용할 것입니다. 따라서 각 실험에서 리포트되는 정확도는 실제로 검증 정확도이지 테스트 셋에 대한 정확도는 아닙니다. 좋은 소식은 검증 셋에 아무 많은 데이터를 사용하지 않아도 된다는 것입니다. 추정에서 불확실은 $O(n^{-\frac{1}{2}})$ 오더로 표현됩니다.


### $K$-겹 교차 검증($K$-Fold Cross-Validation)

학습 데이터가 부족한 경우, 적당한 검증 셋을 구성하기에 충분한 데이터를 만들기 위해 데이터를 확보해 두기 어렵습니다. 이 문제에 대한 유명한 해결책 중 하나는 *$K$-겹 교차 검증* 을 사용하는 것입니다. 여기서 우리는 원래 학습 데이터를 겹치지 않는 $K$ 개의 부분 집합으로 나눕니다. 그리고, 모델 학습과 검증은 $K$ 번 수행하는 데, 매 학습은 $K - 1$ 개의 서브셋을 이용하고, 검증은 나머지 한 개의 서브셋(이번 라운드에 학습에 사용되지 않은 데이터들)에 수행합니다. 최종적으로 학습과 검증 오류율은 $K$ 번의 실험들의 결과에 대한 평균으로 추정합니다.


## 언더피팅과 오버피팅

학습 오류와 검증 오류를 비교할 때, 우리는 두 가지 일반적인 상황을 염두해야 합니다. 첫번째, 학습 오류와 검증 오류 모두 크지만, 두 오류의 차이가 작은 경우를 주의해야 합니다. 만약 모델이 학습 오류를 줄이지 못한다면, 이는 우리의 모델이 우리가 모델을 시도하는 패턴을 잡아내기에는 너무 간단하다는 것(즉, 불충분한 표현되는 것)을 의미합니다. 더군다나, 학습 오류와 검증 오류의 *일반화 간격* 이 작기 떄문에, 더 복잡한 모델을 사용하면 이 문제에서 벗어날 수 있다고 믿을 수 있는 이유가 있습니다. 이 현상은 언터피팅이라고 알려져 있습니다.

반면 앞에서 논의했 듯이, 심각한 오버피팅을 알려주는 학습 오류가 검증 오류보다 굉장히 낮은 경우를 염두해야 합니다. 오버피팅이 항상 나쁜 것은 아님을 유의하세요. 특히 딥러닝의 경우에는 최고의 예측 모델은 종종 검증 데이터보다 학습 데이터에 대해서 아주 잘 동작한다는 것은 잘 알려진 사실입니다. 결국에는 보통 우리는 학습 오류와 검증 오류의 차이 보다는 검증 오류에 더 많은 관심을 둡니다.

오퍼피팅 또는 언터피팅이 되는지는 모델의 복잡도와 가용한 학습 데이터셋의 크기에 의존됩니다. 두 주제에 대해서 알아보겠습니다.

### 모델 복잡도

오버피팅과 모델 복잡도에 대한 전통적인 직관을 설명하기 위해서, 다항식을 이용한 예들 들어보겠습니다. 단일 특성 $x$ 와 이에 대한 실수 레이블 $y$ 로 구성된 학습 데이터가 주어졌고, 우리는 레이블 $y$ 를 추정하는 $d$ 차원 다항식 $$\hat{y}= \sum_{i=0}^d x^i w_i$$ 을 찾으려고 합니다. 이는 특성들이 $x$ 의 거듭제곱이고,  $w_i$ 는 모델의 가중치, 그리고 모든 $x$ 에 대해서  $x^0 = 1$ 이기 때문에 편향은  $w_0$ 로 주어지는 단지 선형 회귀 문제입니다.

고차원 다항식은 저차원 다항식보다 더 복잡한데 그 이유는 고차원 다항식은 더 많은 파라미터를 갖고, 모델 함수 선택 범위가 더 넓기 때문입니다. 학습 데이터가 고정되어 있으면, 고차원 다항식 함수는 저차원 다항식의 경우보다 낮은 (최악의 경우에는 같은) 학습 오류를 달성합니다. 사실 데이터 포인트들이 서로 다른 $x$ 값을 갖는 경우, 데이터 포인트 개수와 같은 차원의 다항식은 학습 세트에 완벽하게 학습될 수 있습니다. 아래 그림은 다항식의 차원과 언더피팅, 오버피팅의 관계를 시각화합니다.


![Influence of Model Complexity on Underfitting and Overfitting](../img/capacity_vs_error.svg)

### 데이터셋의 크기

크게 염두해야 할 것은 데이터 세트의 크기입니다. 모델을 고정했을 때, 학습 데이터셋으로 적은 샘플을 사용할 수록, 오버피팅 될 가능성이 더 높아집니다. (그리고 더 심각하게 됩니다.) 학습 데이터 양을 증가시키면 일반화 오류를 일반적으로 내려갑니다. 더군다나, 일반적으로 더 많은 데이터는 해가되지 않습니다. 정해진 과제와 데이터 *분포* 의 경우, 일반적으로 모델 복잡도와 데이터 세트 크기 사이에 관계가 존재합니다. 데이터가 더 많으면, 우리는 더 복잡한 모델을 학습시킬려고 유리하게 시도할 수 있습니다. 많은 과제들의 경우, 수천 개의 학습 데이터가 있을 때만 딥 러닝은 선형 모델들보다 더 잘 동작합니다. 딥 러닝의 현재의 성공의 일부는 인터넷 회사들, 저렴한 저장소, 연결된 디바이스, 경제의 폭넒은 디지털화에 따른 대량 데이터의 풍부함으로 이야가할 수 있습니다.


## 다항식 회귀

이제 우리는 다항식을 데이터에 학습시키면서 이 개념들을 인터엑트브하게 살펴보겠습니다. 시작에 앞서 유용한 패키지들을 import 합니다.

```{.python .input  n=1}
import sys
sys.path.insert(0, '..')

%matplotlib inline
import d2l
from mxnet import autograd, gluon, nd
from mxnet.gluon import data as gdata, loss as gloss, nn
```

### 데이터셋 생성하기

우선 데이터가 필요합니다. 주어진 $x$ 에 대해서, 다음 3차원 방적식을 사용해서 학습 데이터와 테스트 데이터로 사용할 레이블(label) 만들겠습니다.

$$y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where }
\epsilon \sim \mathcal{N}(0,0.1)$$

노이즈 항목  $\epsilon$ 는 평균이 0이고 표준 편차가 0.1인 표준 분포를 따릅니다. 학습 데이터 세트와 테스트 데이터 세트로 사용할 100개의 샘플을 각각 생성합니다.

```{.python .input  n=2}
maxdegree = 20  # Maximum degree of the polynomial
n_train, n_test = 100, 100  # Training and test data set sizes
true_w = nd.zeros(maxdegree)  # Allocate lots of empty space
true_w[0:4] = nd.array([5, 1.2, -3.4, 5.6])

features = nd.random.normal(shape=(n_train + n_test, 1))
features = nd.random.shuffle(features)
poly_features = nd.power(features, nd.arange(maxdegree).reshape((1, -1)))
poly_features = poly_features / (
    nd.gamma(nd.arange(maxdegree) + 1).reshape((1, -1)))
labels = nd.dot(poly_features, true_w)
labels += nd.random.normal(scale=0.1, shape=labels.shape)
```

최적화를 위해서, 그래디언트(gradient), 손실(loss) 등이 큰 값을 갖는 것을 피해야합니다. `poly_features` 에 저장되는 단항들이 $x^i$ 에서 $\frac{1}{i!} x^i$ 로 스캐일을 조정하는 이유입니다. 이렇게 하면 큰 차원 $i$ 의 값들이 아주 커지는 것을 방지할 수 있습니다. 팩토리얼은 Gluon의 Gamma 함수를 이용해서 구현합니다. ( $n! = \Gamma(n+1)$)

생성된 데이터 셋에서 처음 두 샘플을 확인해봅니다. 

값 1도 기술적으로 보면 하나의 특성(feature)로, bias에 대한 상수 특성(feature)라고 볼 수 있습니다.

```{.python .input  n=3}
features[:2], poly_features[:2], labels[:2]
```

### 모델 정의, 학습, 그리고 테스트

우선 그래프를 그리는 함수 `semilogy`  를 정의합니다. $y$ 축은 로그(logarithm) 단위를 사용합니다.

```{.python .input  n=4}
# This function has been saved in the d2l package for future use
def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,
             legend=None, figsize=(3.5, 2.5)):
    d2l.set_figsize(figsize)
    d2l.plt.xlabel(x_label)
    d2l.plt.ylabel(y_label)
    d2l.plt.semilogy(x_vals, y_vals)
    if x2_vals and y2_vals:
        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=':')
        d2l.plt.legend(legend)
```

생성된 데이터를 이용해서 여러 복잡도를 갖는 모델들을 학습시킬 것이기 때문에, 모델 정의를 `fit_and_plot` 함수에 전달하도록 하겠습니다. 다항 함수에 대한 학습과 테스트 단계는 softmax 회귀(regression)과 비슷합니다.

```{.python .input  n=5}
num_epochs, loss = 200, gloss.L2Loss()

def fit_and_plot(train_features, test_features, train_labels, test_labels):
    net = nn.Sequential()
    # Switch off the bias since we already catered for it in the polynomial
    # features
    net.add(nn.Dense(1, use_bias=False))
    net.initialize()
    batch_size = min(10, train_labels.shape[0])
    train_iter = gdata.DataLoader(gdata.ArrayDataset(
        train_features, train_labels), batch_size, shuffle=True)
    trainer = gluon.Trainer(net.collect_params(), 'sgd',
                            {'learning_rate': 0.01})
    train_ls, test_ls = [], []
    for _ in range(num_epochs):
        for X, y in train_iter:
            with autograd.record():
                l = loss(net(X), y)
            l.backward()
            trainer.step(batch_size)
        train_ls.append(loss(net(train_features),
                             train_labels).mean().asscalar())
        test_ls.append(loss(net(test_features),
                            test_labels).mean().asscalar())
    print('final epoch: train loss', train_ls[-1], 'test loss', test_ls[-1])
    semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',
             range(1, num_epochs + 1), test_ls, ['train', 'test'])
    print('weight:', net[0].weight.data().asnumpy())
```

### 3차 다항 함수 피팅 (일반)

우선, 데이터를 생성한 것과 같은 3차원 다항 함수를 이용해보겠습니다. 테스트 데이터를 이용해서 얻은 모델의 오류는 낮게 나오는 것이 보여집니다.  학습된 모델 파라메터 역시 실제 값  $w = [5, 1.2, -3.4, 5.6]$ 과 비슷합니다.

```{.python .input  n=6}
num_epochs = 1000
# Pick the first four dimensions, i.e. 1, x, x^2, x^3 from the polynomial
# features
fit_and_plot(poly_features[:n_train, 0:4], poly_features[n_train:, 0:4],
             labels[:n_train], labels[n_train:])
```

### 선형 함수 피팅 (언더피팅)

선형 함수 학습을 다시 살펴봅시다. 초기 에포크에서 감소한 후로 모델의 학습 오류율이 더 감소하기 어려와 졌습니다. 마지막 에포크 반복을 마친 후에 학습 오류율은 여전히 높습니다. 비선형 패턴을 학습 시킬 때 선형 모델을 사용하면 언더피팅되기 쉽습니다.

```{.python .input  n=7}
num_epochs = 1000
# Pick the first four dimensions, i.e. 1, x from the polynomial features
fit_and_plot(poly_features[:n_train, 0:3], poly_features[n_train:, 0:3],
             labels[:n_train], labels[n_train:])
```

### 부족한 학습 (오버피팅)

이번에는 너무 높은 차원의 다항식을 사용해서 모델을 학습시켜보겠습니다. 여기서는, 차원이 높은 계수들의 값이 0에 가까워야 한다는 것을 학습하기에는 데이터가 충분하지 않습니다. 그 결과 너무 복잡한 모델은 학습 데이터에 있는 노이즈에 영향을 받기 너무 쉽습니다. 물론 학습 오류는 이제 낮아질 것이나 (만약 정확한 모델을 갖을 때 보다 더 낮음), 테스트 오류는 높을 것입니다. 

다른 모델의 복잡도 (`n_degreee`)와 학습 셋 크기(`n_subset`)를 적용해서 어떤 일이 발생하는지에 대한 직감을 얻어보세요.

```{.python .input  n=8}
num_epochs = 1000
n_subset = 100  # Subset of data to train on
n_degree = 20   # Degree of polynomials
fit_and_plot(poly_features[1:n_subset, 0:n_degree],
             poly_features[n_train:, 0:n_degree], labels[1:n_subset],
             labels[n_train:])
```

다음 장들에서 오버피팅 문제들을 계속 논의하고, 이를 해결하기 위한 가중치 감쇠와 드롭아웃(dropout) 과 같은 방법을 알아보겠습니다.

## 요약

* 일반화 오류율은 학습 오류율을 이용해서 추정될 수 없기 때문에, 단순히 학습 오류율을 줄이는 것이 일반화 오류를 줄이는 것을 의미하지 않습니다. 머신 러닝 모델은 일반화 오류를 줄이기를 통해서 오버피팅에 조심스럽게 대비 해야합니다.
* 검증 셋은 모델 선택에 사용됩니다. (너무 남용되지 않는다는 가정에서)
* 언더피팅은 모델이 학습 오류를 줄이지 못하는 상황을 의미하고, 오버피팅은 모델 학습 오류가 테스트 데이터의 오류보다 훨씬 작은 경우를 의미합니다.
* 우리는 적절한 모델의 복잡성을 선택해야하고, 부족한 학습 샘플을 이용하는 것을 피해야합니다.

## 문제

1. 다항 회귀 문제를 정확하게 풀 수 있나요? 힌트 - 선형대수를 이용합니다.
1. 다항식에 대한 모델 선택에 대해서
    - 학습 오류와 모델 복잡도(다항식의 차원 수)를 도식화해보세요. 무엇이 관찰되나요?
    - 이 경우 테스트 오류를 도식화해보세요.
    - 같은 그래프를 데이터 양에 따라서 그려보세요.
1. 다항식의 특성(feature)  $x^i$ 에 적용한 정규화 $1/i!$ 를 제거하면 어떤 일이 일어날까요? 다른 방법으로 이를 해결할 수 있나요?
1. 학습 오류를 0으로 줄이기 위해서 몇 차원을 사용하나요?
1. 일반화 오류를 0으로 줄이는 것이 가능한가요?

## QR 코드를 스캔해서 [논의하기](https://discuss.mxnet.io/t/2341)

![](../img/qr_underfit-overfit.svg)
