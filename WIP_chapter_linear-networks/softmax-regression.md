# Softmax 회귀(regression)

우리는 앞 두 절에서 선형 회귀를 [모든 것을 직접 구현](linear-regression-scratch.ipynb) 한 후, 대부분의 반복적인 일을 자동화하기 위해서 [Gluon을 이용해서 구현](linear-regression-gluon.ipynb)하는 것을 했습니다.

회귀(regression)는 몇 개인지, 얼마인지 등에 대한 답을 구할 때 사용하는 도구로, 예를 들면 집 가격이 얼마인지, 어떤 야구팀이 몇 번 승리를 할 것인지 등을 예측하는데 사용할 수 있는 방법입니다. 다른 예로는, 환자가 몇 일 만에 퇴원할 것인지 예측하는 것도 회귀(regression) 문제입니다. 

현실에서 우리는 분류의 문제에 더 자주 관심을 갖습니다: 즉, 얼마나 많이를 묻기보다는 어떤 것인지를 더 묻습니다.

* 이 이메일이 스팸 폴더에 속하는지 아니면 받은 편지함에 속할지
* 이 고객이 구독 서비스에 *가입 할 것인지* 아니면 *가입하지 않을 것인지*?
* 이미지에 있는 객체가 무엇인지 (원숭이, 강아지, 고양이, 닭 등)
* 사용자가 다음에 어떤 영화를 볼 것인지

우리는 *분류* 라는 단어를 약간은 다른 두 가지 문제에 대해서 사용합니다: (i) 예제들을 카테고리에 *하드* 할당(hard assignment)을 하는 것에 관심이 있는 경우와,  (ii) *소프트* 할당(soft assignment)을 하고자 하는 경우. 즉, 각 카테고리에 속할 *확률* 을 할당하는 것. 이 두가지에 대한 분류를 명화하게 하지 않은 이유는 종종 하드 할당을 위해서 여전히 소프트 할당을 하는 모델을 이용하기 때문입니다.

## 분류 문제들

간단한 예제를 들어서 시작하겠습니다. 다소 인위적인 이미지 분류 문제를 생각해봅니다. 입력은 2x2 회색 이미지입니다. 각 픽셀 위치는 하나의 스칼라로 표현할 수 있고, 즉 각 이미지는 4개의 특성(feature)들 $x_1, x_2, x_3, x_4$ 로 표현됩니다. 그리고 각 이미지는 "고양이", "닭", 그리고 "개", 3개 종류 중 하나에 속합니다.

우선 우리는 레이블을 표현 방법을 선택해야 합니다. 쉽게는 두 가지 방법이 있습니다. 가장 자연스러운 방법은  $y \in \{1, 2, 3\}$ 를 이용해서 각 숫자가 {개, 고양이, 닭}를 의미하게 하는 것입니다. 이 방법은 이런 정보를 컴퓨터에 *저장* 하는 좋은 방법입니다. 예를 들어 {아기, 어린이, 청소년, 어른} 중에 하나를 예측하는 것처럼 카테고리들이 자연스러운 순서가 있다면, 이 문제를 회귀 문제로 정의하고, 레이블이 이 형태로 사용하는 것이 의미가 있습니다.

하지만, 일반적인 분류 문제에서는 클래스 사이에 자연스러운 순서가 있지 않습니다. 이와 같은 문제를 다루기 위해서 통계학자들은 카테고리 데이터를 표현하는 다른 방법을 만들었습니다: 이는 원-핫-인코딩(one-hot-encoding) 입니다. 즉, 모든 가능한 카테고리에 대해서 하나의 컴포넌트를 갖는 벡터를 사용합니다. 이 예에서는 *카테고리*에 해당하는 컴포넌트를 1로 설정하고, 나머지 컴포넌트는 모두 0으로 설정합니다.

$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}$$

우리의 예에서는 $y$ 는 (1,0,0)은 고양이를, (0,1,0)은 닭은, (0,0,1)은 강아지를 의미하는 3차원 벡터가 될 것입니다.

## 네트워크 아키텍처

여러 클래스를 예측하기 위해서는 여러 출력(카테고리당 한 개)을 갖는 모델이 필요합니다. 이것이 분류 모델과 회귀 모델의 주요 차이점 중에 하나 입니다. 선형 모델로 분류의 문제를 해결하기 위해서는 결과 만큼의 선형 함수가 필요합니다. 즉, 각 출력에 대해서 한 개의 선형 함수가 있어야 합니다. 우리 예제에서는 4개의 특성과 3개의 결과 카테고리가 있기 때문에 가중치를 표현하는 12개의 스칼라 ($w$ 와 아래첨자로 표기)와 편향을 위한 3개 스칼라($b$ 와 아래첨자로 표기)가 필요합니다. 각 입력에 대한 3개의 출력  $o_1, o_2$, $o_3$ 은 다음과 같이 계산합니다.
$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\
o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\
o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3.
\end{aligned}
$$

이 계산을 뉴럴 네트워크 다이어그램으로 아래와 같이 표현할 수 있습니다. 선형 회귀의 경우와 같이 softmax 회귀 또한 단층 뉴럴 네트워크입니다. 각 결과 t, $o_1, o_2$, $o_3$ 는 모든 입력  $x_1$, $x_2$, $x_3$, $x_4$ 값에 의해서 계산되기 때문에, softmax 회귀의 출력층은 역시 완전 연결층으로 표현될 수 있습니다.

![Softmax 회귀는 단일층 뉴럴 네트워크입니다.](../img/softmaxreg.svg)

## Softmax 연산

모델을 더 간결하게 표현하기 위해서 선형대수 표기법을 사용할 수 있습니다. 벡터 형태를 사용하면, 모델을 $\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$ 로 적을 수 있습니다. 이는 수학자나 코드를 작성할 때 더 적합한 표기법입니다. 즉, 모든 가중치들을 모아서  $3\times4$ 행렬로 만들었고, 어떤 예제  $\mathbf{x}$ 가 주어지면, 결과는 가중치와 입력의 행렬-벡터곱과 편향  $\mathbf{b}$ 를 더해서 얻습니다.

하드 분류를 원할 경우, 이 결과값들을 이산 예측(discrete prediction)으로 바꿀 필요가 있습니다. 이를 위한 가장 직관적인 방법은 결과값($o_i$)들을 각 카테고리에 대한 상대적인 확신 수준으로 취급하는 것입니다. 이 중에 가장 큰 값을 갖는 클래스를 선택($\operatorname*{argmax}_i o_i$)해서 예측 결과로 사용합니다. 예를 들어, $o_1$, $o_2$, $o_3$ 이 각각 0.1, 10, 0.1 일 경우, 우리는 2번째 카테고리, 즉 "닭",으로 예측하는 것입니다.

하지만 결과층의 값들을 직접 사용하는 데에는 몇 가지 문제가 있습니다. 첫번째 문제는 결과층의 값의 범위가 불특정하기 때문에 각 값에 대한 의미를 부여하기 어렵습니다. 즉, 앞의 예제에서 결과값 10은 이미지 카테고리가 *닭* 이라는 결과에 대해서 *매우 확신한* 것처럼 보이게 할 수 있습니다. 하지만, 얼마나 확실한 것일 까요? 이미지의 카테고리가 "개"인 경우 보다 약 100 배정도 "닭"일 수 있다는 것을 의미할까요 아니면 그보다 덜 확신하는 것일까요?

더군다나 이 모델을 어떻게 학습을 시켜야 할까요? argmax 값이 레이블과 일치하는 경우, 오류가 전혀 없다는 것을 의미합니다. 반면에 argmax가 레이블과 같이 않다면, 가중치에 대한 어떤 작은 변화도 오류를 감소시키지 못 합니다. 즉, 그레디언트 기반의 학습을 사용할 수 없게됩니다.

우리는 결과값이 확률값으로 사용될 수 있기를 원하는데, 그러기 위해서는 새로운(본적이 없는) 데이터에 대해서 확률이 0보다 크게 나오고, 그 합은 1되도록 보장하는 방법이 필요합니다. 더군다나, 모델이 실제로 *확률* 을 추정할 수 있도록 하는 학습 목표(objective)가 필요합니다. 운좋게도 통계학자들은 정확히 이 일을 하는 소프트맥스 로지스틱 회귀(softmax logistic regression)라고 불리는 모델을 만들어놨습니다.

결과들이 0보다 크면서 합이 1이고, 모델이 미분가능하게 하기 위해서, 우리는 모델의 선형 부분의 결과를 비선형 함수, *softmax* 에 대입니다.
$$
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o}) \text{ , 이 때 }
\hat{y}_i = \frac{\exp(o_i)}{\sum_j \exp(o_j)}
$$

모든 $i$ 에 대해서  $0 \leq \hat{y}_i \leq 1$  이고  $\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$ 를 만족하는 것을 쉽게 확인할 수 있습니다. 따라서, $\hat{y}$ 은 적절한 확률 분포이고, $o$ 값은 쉽게 측정할 수 있는 값으로 간주할 수 있습니다. 아래 공식은 가장 가능성 있는 클래스를 찾아줍니다.
$$
\hat{\imath}(\mathbf{o}) = \operatorname*{argmax}_i o_i = \operatorname*{argmax}_i \hat y_i
$$

요약하면, 소프트맥스 연산은 입력의 순서를 보존하고, 단순한 *argmax* 모델과는 다르게 예측된 카테고리를 변경하지 않습니다. 하지만, 이는 결과  $\mathbf{o}$ 에 적당한 의미를 부여합니다: 즉, 그 값들은 각 카테고리에 대한 확률을 결정하는 사전-소프트맥스 값입니다. 벡터 표기법으로 요약하면,  ${\mathbf{o}}^{(i)} = \mathbf{W} {\mathbf{x}}^{(i)} + {\mathbf{b}}$,  ${\hat{\mathbf{y}}}^{(i)} = \mathrm{softmax}({\mathbf{o}}^{(i)})$ 이 됩니다.

## 미니 배치를 위한 벡터화

앞에서 언급했듯이, 연산 효율성을 높이고 GPU를 잘 이용하기 위해서는 데이터의 미니-배치에 대한 벡터 연산을 사용합니다. 차원이 $d$ 이고 배치 크기가 $n$ 인 미니-배치 $\mathbf{X}$ 가 있고, 결과들은 $q$ 개의 카테고리 중 하나에 속한다고 가정하겠습니다. 이 때, 미니배치 특성 $\mathbf{X}$ 는  $\mathbb{R}^{n \times d}$ 에 속하고, 가중치는 $\mathbf{W} \in \mathbb{R}^{d \times q}$ , 편향은  $\mathbf{b} \in \mathbb{R}^q$ 이 됩니다.
$$
\begin{aligned}
\mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b} \\
\hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O})
\end{aligned}
$$

이 방법을 사용하면 샘플을 한 개씩 처리할 때 사용했던 행렬-벡터곱과 대비해서 대부분의 연산을 행렬-행렬곱 $\mathbf{W} \mathbf{X}$ 으로 하기 때문에 연산 속도를 높이게 됩니다. 소프트맥스는 $\mathbf{O}$ 의 모든 항목들에 대해서 지수를 취하고, 합으로 정규화를 해서 구합니다.

## 손실 함수(loss function)

확률 결과를 출력하는 방법을 정의했으니, 이 값이 얼마나 정확한지를 측정하는 값으로 변환하는 것이 필요합니다. 즉, 손실 함수(loss function)가 필요합니다. 선형 회귀에서 사용했던 것과 동일한 개념을 사용하는데, 이는 가능도 최대화(likelihood maxmization)이라고 합니다.

### 로그 가능도(Log-Likelihood)

소프트맥스 함수는 결과  $\mathbf{o}$ 를 여러 결과들에 대한 확률, $p(y=\mathrm{cat}|\mathbf{x})$, 들의 벡터로 변환합니다. 이는, 예측된 값이 얼마나 잘 예측하고 있는지를 확인하는 것으로 실제 값과 예측 결과에 대한 비교를 할 수 있습니다.
$$
p(Y|X) = \prod_{i=1}^n p(y^{(i)}|x^{(i)})
\text{ 이고, 그러므로 }
-\log p(Y|X) = \sum_{i=1}^n -\log p(y^{(i)}|x^{(i)})
$$

레이블을 잘 예측하는 것은 $p(Y|X)$ 를 최대화 (또는 $-\log p(Y|X)$ 를 최소화)하는 것을 의미합니다. 이를 통해서 손실 함수(loss function)를 다음과 같이 정의할 수 있습니다. (표기를 간단하게 하기 위해서 $i$ 는 제외했습니다.)
$$
l = -\log p(y|x) = - \sum_j y_j \log \hat{y}_j
$$

여기서  $\hat{y} = \mathrm{softmax}(\mathbf{o})$ 이고, 벡터 $\mathbf{y}$ 는 해당하는 레이블이 아닌 위치에는 모두 0을 갖습니다. (예를 들면 (1,0,0)). 따라서, 모든 $j$ 에 대한 합을 하면, 하나의 항목만 남게 됩니다. 모든 $\hat{y}_j$ 는 확률값이기 때문에, 이에 대한 로그를 적용한(logarithm) 값은 0보다 커질 수 없습니다. 그 결과, 주어진 x에 대해서 y를 잘 예측하는 경우라면 (즉,  $p(y|x) = 1$), 손실 함수(loss function)는 최소화될 것입니다.

## Softmax와 미분(derivative)

Softmax와 이에 대한 손실(loss)는 많이 사용되기 때문에, 어떻게 계산되는지 자세히 살펴볼 필요가 있습니다.  $o$ 를 손실 $l$ 의 정의에 대입하고, softmax의 정의를 이용하면, 다음과 같이 표현을 얻습니다.
$$
l = -\sum_j y_j \log \hat{y}_j = \sum_j y_j \log \sum_k \exp(o_k) - \sum_j y_j o_j
= \log \sum_k \exp(o_k) - \sum_j y_j o_j
$$

어떤 일이 일어나는지 더 살펴보기 위해서, 손실 함수(loss function)를 $o$ 에 대해서 미분을 해보면 아래 공식을 유도할 수 있습니다.
$$
\partial_{o_j} l = \frac{\exp(o_j)}{\sum_k \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j = \Pr(y = j|x) - y_j
$$

다르게 설명해보면, 그래디언트(gradient)는 확률 $p(y|x)$ 로 표현되는 모델이 실제 클래스에 할당하는 확률과 $y$ 로 표현되는 실제 일어난 것과의 차이입니다.이는 회귀 문제에서 보았던 것과 아주 비슷합니다. 회귀 문제에서 그래디언트(gradient)가 관찰된 실제 값 $y$ 와 예측된 값 $\hat{y}$ 의 차이로 계산되었습니다. 이는 우연이 아닙니다. [exponential 계열](https://en.wikipedia.org/wiki/Exponential_family)의 모델의 경우에는, 로그 가능도(log-likelihood)의 그래디언트(gradient)는 정확하게 이 항목으로 주어집니다. 이로 인해서 실제 환경에서 그래디언트(gradient)를 구하는 것이 쉬워집니다.

### 크로스-엔트로피 손실(cross-entropy loss)

자 이제는 하나의 결과에 대한 관찰을 하는 경우가 아니라, 결과들에 대한 전체 분포를 다루는 경우를 생각해봅시다.  $y$ 에 대한 표기를 이전과 동일하게 사용할 수 있습니다. 오직 다른 점은 (0,0,1) 과 같이 이진(binary) 값을 갖는 것이 아니라 (0.1, 0.2, 0.7)과 같이 일반적인 확률 벡터를 사용한다는 것입니다. 손실 $l$ 의 정의도 동일한 수학을 사용하지만, 이에 대한 해석은 조금 더 일반적입니다. 레이블들의 분포에 대한 손실의 기대값을 의미합니다.
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_j y_j \log \hat{y}_j
$$

이렇게 정의된 손실는 크로스-엔트로피 손실(cross-entropy loss)이라고 부르며, 이것은 다중 클래스 분류에 가장 흔히 사용되는 손실 입니다. 이 이름에 대해서 알아보기 위해서는 정보 이론(information theory)에 대한 설명이 필요하며, 지금부터 설명하겠습니다. 다음 내용은 넘어가도 됩니다.

## 정보 이론(Information theory) 기초

정보 이론(information theory)는 정보 (또는 데이터)를 가능한 한 간결한 형식으로 인코딩, 디코딩, 전송, 및 변조하는 문제를 다룹니다.

### 엔트로피(Entropy)

데이터 (또는 난수)에 몇개의 정보 비트들이 담겨있는지가 중요한 개념입니다. 이는 분표 $p$ 의 [entropy](https://en.wikipedia.org/wiki/Entropy)로 다음과 같이 수치화할 수 있습니다.
$$
H[p] = \sum_j - p(j) \log p(j)
$$

정보 이론의 근본적인 이론 중에 하나로 분포 $p$ 로부터 임의로 추출된 데이터를 인코드하기 위해서는 최소  $H[p]$ 개의 'nat'이 필요하다는 것이 있습니다. 여기서 'nat'은 비트와 동일하나, 베이스(base) 2가 아니라 베이스(base) $e$ 를 이용합니다. 즉, 1 nat은 $\frac{1}{\log(2)} \approx 1.44$  비트이고,  $H[p] / 2$ 는 종종 이진 앤트로피(binary entropy)라고 불립니다.

조금 더 이론적으로 들어가보겠습니다. $p(1) = \frac{1}{2}$ 이고,  $p(2) = p(3) = \frac{1}{4}$ 인 분포를 가정하겠습니다. 이 경우, 이 분포에서 추출한 데이터에 대한 최적의 코드를 굉장히 쉽게 설계할 수 있습니다. 즉, 1의 인코딩은 `0`, 2와 3에 대한 인코딩은 각 각 `10`, `11` 로 정의하면 됩니다. 예상되는 비트 개수는  $1.5 = 0.5 * 1 + 0.25 * 2 + 0.25 * 2$ 이고, 이 숫자는 이진 앤트로피(binary entropy) $H[p] / \log 2$ 와 같다는 것을 쉽게 확인할 수 있습니다.

### Kullback Leibler Divergence

두 분포간에 차이를 측정하는 방법 중에 하나로 앤트로피(entropy)를 이용하는 방법이 있습니다. $H[p]$ 는 분포 $p$를 따르는 데이터를 인코드하는데 필요한 최소 비트 수를 의미하기 때문에, 틀린 분포 $q$ 에서 뽑았을 때 얼마나 잘 인코딩이 되었는지를 물어볼 수 있습니다. $q$ 를 인코딩하는데 추가로 필요한 비트 수는 두 분표가 얼마나 다른지에 대한 아이디어를 제공합니다. 직접 계산해보겠습니다. 분포 $q$ 에 대해 최적인 코드를 이용해서 $j$ 를 인코딩하기 위해서는 $-\log q(j)$ nat이 필요하고,  $p(j)$ 인 모든 경우에서 이를 사용하면, 다음 식을 얻습니다.
$$
D(p\|q) = -\sum_j p(j) \log q(j) - H[p] = \sum_j p(j) \log \frac{p(j)}{q(j)}
$$

$q$ 에 대해서  $D(p\|q)$ 를 최소화하는 것은 크로스-엔트로피 손실(cross-entropy loss)을 최소화하는 것과 같습니다. 이는 $q$ 에 의존하지 않는 $H[p]$ 를 빼버리면 바로 얻을 수 있습니다. 이를 통해서 우리는 softmax 회귀(regression)는 예측된 값  $\hat{y}$ 이 아니라 실제 레이블 $y$ 를 봤을 때 얻는 놀라움(비트 수)을 최소화하려는 것임을 증명했습니다.

## 모델 예측 및 평가

학습된 softmax 회귀(regression) 모델을 사용하면, 새로운 특성(feature)가 주어졌을 때, 각 결과 카테고리에 속할 확률값을 예측할 수 있습니다. 일반적으로는 가장 크게 예측된 확률값을 갖는 카테고리를 결과 카테고리라고 정의합니다. 실제 카테고리 (label)와 일치하는 경우에 예측이 정확하다고 합니다. 다음에는 모델의 성능을 평가하는 방법으로 accuracy 정확도를 사용할 예정입니다. 이는 정확하게 예측한 개수와 전체 예측의 개수의 비율과 같습니다. 

## 요약

* 벡터를 확률로 변환하는 소프트맥스 연산을 알아봤습니다.
* 소프트맥스 회귀(softmax regression)은 분류의 문제에 적용할 수 있습니다. 소프트맥스 연산을 이용해서 얻은 결과 카테고리의 확률 분포를 이용합니다.
* 크로스 엔트로피(cross entropy)는 두 확률 분포의 차이를 측정하는 좋은 방법입니다. 이는 주어진 모델이 데이터를 인코드하는데 필요한 비트 수를 나타냅니다.

## 연습문제

1. Kullback-Leibler divergence $D(p\|q)$ 가 모든 음수가 분포 $p$, $q$ 에 대해서 음수가 아님을 증명하세요. 힌트 - Jensen의 부등식을 이용하세요. 예를 들여,  $-\log x$ 가 볼록 함수(convex function)이라는 사실을 사용하세요.
1. $\log \sum_j \exp(o_j)$ 가  $o$에서 볼록 함수(convex function)임을 증명하세요.
1. 지수승 집단과 소프트맥스의 관계를 심도있게 알아볼 수 있습니다.
    * softmax에 대한 크로스 엔트로피 손실 $l(y,\hat{y})$ 의 이차 미분을 계산하세요.
    *  $\mathrm{softmax}(o)$ 로 주어지는 분포의 분산을 계산하고, 위에서 계산한 이차 이분과 같음을 증명하세요.
1. 동일한 확률로 일어나는 3개 클래스가 있다고 가정합니다. 즉, 확률 벡터가  $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$ 입니다.
    * 이 문제를 위한 이진 코드(binary code)를 설계하고자 하면 어떤 문제가 있을까요? 앤트로피의 하한(lower bound)를 필요한 비트수로 같게할 수 있나요?
    * 더 좋은 코드를 설계할 수 있나요? 힌트 - 두 독립적인 관찰을 인코딩하려면 어떤일이 생기나요? $n$개의 관찰을 연관해서 인코드하면 어떨까요?
1. Softmax는 위에서 소개된 매핑에 대한 잘못된 이름입니다 (하지만 딥러닝에서 많은 사람들이 쓰고 있습니다.) 실제 softmax는 $\mathrm{RealSoftMax}(a,b) = \log (\exp(a) + \exp(b))$ 로 정의됩니다.
    * $\mathrm{RealSoftMax}(a,b) > \mathrm{max}(a,b)$ 임을 증명하세요.
    * $\lambda > 0$ 일 경우, 모든  $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b)$ 에 대해서 이것이 성립함을 증명하세요
    * $\lambda \to \infty$ 이면,  $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a,b)$ 임을 증명하세요.
    * soft-min은 어떻게 생겼을까요?
    * 이를 두개 이상의 숫자들로 확장해보세요.

## QR 코드를 스캔해서 [논의하기](https://discuss.mxnet.io/t/2334)

![](../img/qr_softmax-regression.svg)
